{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARcBj-pgtuus",
        "outputId": "d381af16-2640-4f22-ea89-bfbc85a237bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q-SKy5-t8Tg",
        "outputId": "4c026d22-88bd-46f5-f209-3610045b03dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, context):\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    word_tokens = tokenizer.tokenize(word)\n",
        "    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
        "    word_index = inputs.input_ids[0].tolist().index(word_ids[0])\n",
        "    return outputs.last_hidden_state[0, word_index, :].numpy()\n",
        "\n",
        "def context_transformation(context, T_type):\n",
        "    if T_type == \"tense_change\":\n",
        "        # Simple tense change: present to past\n",
        "        return context.replace(\"is\", \"was\")\n",
        "    elif T_type == \"add_adjectives\":\n",
        "        # Add an adjective before the first noun\n",
        "        words = context.split()\n",
        "        for i, word in enumerate(words):\n",
        "            if word.lower() in [\"house\", \"car\", \"tree\", \"book\"]:  # Example nouns\n",
        "                words.insert(i, \"beautiful\")\n",
        "                break\n",
        "        return \" \".join(words)\n",
        "    else:\n",
        "        return context  # No transformation if type is not recognized\n",
        "\n",
        "def meaning_transformation_F(M_w_c, T):\n",
        "    # Implement the F function\n",
        "    # This is a placeholder linear transformation\n",
        "    np.random.seed(0)  # For reproducibility\n",
        "    A_T = np.random.rand(768, 768)  # For BERT's 768-dim embeddings\n",
        "    b_T = np.random.rand(768)\n",
        "    return np.dot(A_T, M_w_c) + b_T\n",
        "\n",
        "# Test the covariance\n",
        "word = \"house\"\n",
        "original_context = \"The house is big.\"\n",
        "transformed_context = context_transformation(original_context, \"tense_change\")\n",
        "\n",
        "print(f\"Original context: {original_context}\")\n",
        "print(f\"Transformed context: {transformed_context}\")\n",
        "\n",
        "M_w_c = get_embedding(word, original_context)\n",
        "M_w_Tc = get_embedding(word, transformed_context)\n",
        "\n",
        "predicted_M_w_Tc = meaning_transformation_F(M_w_c, \"tense_change\")\n",
        "\n",
        "# Compare M_w_Tc and predicted_M_w_Tc\n",
        "similarity = np.dot(M_w_Tc, predicted_M_w_Tc) / (np.linalg.norm(M_w_Tc) * np.linalg.norm(predicted_M_w_Tc))\n",
        "print(f\"Cosine similarity between actual and predicted transformed embeddings: {similarity}\")\n",
        "\n",
        "# Additional analysis\n",
        "print(f\"\\nEuclidean distance between original and transformed embeddings: {np.linalg.norm(M_w_c - M_w_Tc)}\")\n",
        "print(f\"Euclidean distance between transformed and predicted embeddings: {np.linalg.norm(M_w_Tc - predicted_M_w_Tc)}\")\n",
        "\n",
        "# Visualize the first few dimensions of the embeddings\n",
        "print(\"\\nFirst 10 dimensions of embeddings:\")\n",
        "print(\"Original:    \", M_w_c[:10])\n",
        "print(\"Transformed: \", M_w_Tc[:10])\n",
        "print(\"Predicted:   \", predicted_M_w_Tc[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF5YTfT9twO9",
        "outputId": "e2ce316b-6417-4396-97db-02ee8ed67b0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original context: The house is big.\n",
            "Transformed context: The house was big.\n",
            "Cosine similarity between actual and predicted transformed embeddings: 0.031218016020476986\n",
            "\n",
            "Euclidean distance between original and transformed embeddings: 3.9046308994293213\n",
            "Euclidean distance between transformed and predicted embeddings: 157.35978777902625\n",
            "\n",
            "First 10 dimensions of embeddings:\n",
            "Original:     [ 1.1914905   0.20965098  0.6337387  -0.4308313   0.9291093  -0.42965057\n",
            " -0.0608275   0.21266815 -0.7103103  -0.48505405]\n",
            "Transformed:  [ 1.0322326   0.44214618  0.2158463  -0.39376912  0.7595385  -0.3879153\n",
            "  0.02685729  0.18928054 -0.52789253 -0.50278306]\n",
            "Predicted:    [-1.34185679 -5.98520063  0.53370562 -6.33069443 -3.62910611 -0.52153571\n",
            " -0.58382468 -6.89555114 -0.46753446 -0.76195684]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def normalized_euclidean(a, b):\n",
        "    a_norm = a / np.linalg.norm(a)\n",
        "    b_norm = b / np.linalg.norm(b)\n",
        "    return np.linalg.norm(a_norm - b_norm)\n",
        "\n",
        "def manhattan_distance(a, b):\n",
        "    return np.sum(np.abs(a - b))\n",
        "\n",
        "# Using the same embeddings from your previous result\n",
        "M_w_c = np.array([ 1.1914905, 0.20965098, 0.6337387, -0.4308313, 0.9291093, -0.42965057, -0.0608275, 0.21266815, -0.7103103, -0.48505405])\n",
        "M_w_Tc = np.array([ 1.0322326, 0.44214618, 0.2158463, -0.39376912, 0.7595385, -0.3879153, 0.02685729, 0.18928054, -0.52789253, -0.50278306])\n",
        "\n",
        "# Extend these to 768 dimensions for this example (you would use the full embeddings in practice)\n",
        "M_w_c = np.tile(M_w_c, 77)[:768]\n",
        "M_w_Tc = np.tile(M_w_Tc, 77)[:768]\n",
        "\n",
        "print(f\"Cosine Similarity: {cosine_similarity(M_w_c, M_w_Tc)}\")\n",
        "print(f\"Normalized Euclidean Distance: {normalized_euclidean(M_w_c, M_w_Tc)}\")\n",
        "print(f\"Manhattan Distance: {manhattan_distance(M_w_c, M_w_Tc)}\")\n",
        "\n",
        "# Dimension Reduction\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(np.vstack([M_w_c, M_w_Tc]))\n",
        "pca_distance = np.linalg.norm(embeddings_2d[0] - embeddings_2d[1])\n",
        "print(f\"PCA-reduced Euclidean Distance: {pca_distance}\")\n",
        "\n",
        "# Relative Distance\n",
        "random_vec = np.random.rand(768)\n",
        "relative_distance = np.linalg.norm(M_w_c - M_w_Tc) / np.linalg.norm(M_w_c - random_vec)\n",
        "print(f\"Relative Distance: {relative_distance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnNVO0DxtwmK",
        "outputId": "a250bead-e311-4edc-a28c-6924bdf9f1fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.9646599941462117\n",
            "Normalized Euclidean Distance: 0.26585712649386883\n",
            "Manhattan Distance: 105.23078883\n",
            "PCA-reduced Euclidean Distance: 5.020251923686917\n",
            "Relative Distance: 0.22875415606293376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_transformation(word, context, T_type):\n",
        "    original_context = context\n",
        "    transformed_context = context_transformation(context, T_type)\n",
        "\n",
        "    M_w_c = get_embedding(word, original_context)\n",
        "    M_w_Tc = get_embedding(word, transformed_context)\n",
        "    predicted_M_w_Tc = meaning_transformation_F(M_w_c, T_type)\n",
        "\n",
        "    cosine_sim = np.dot(M_w_Tc, predicted_M_w_Tc) / (np.linalg.norm(M_w_Tc) * np.linalg.norm(predicted_M_w_Tc))\n",
        "    euclidean_dist = np.linalg.norm(M_w_Tc - predicted_M_w_Tc)\n",
        "\n",
        "    return cosine_sim, euclidean_dist\n",
        "\n",
        "# Test with multiple words\n",
        "words = [\"house\", \"car\", \"run\", \"eat\", \"think\"]\n",
        "contexts = [\n",
        "    \"The {} is big.\",\n",
        "    \"The {} is fast.\",\n",
        "    \"They {} quickly.\",\n",
        "    \"They {} healthy food.\",\n",
        "    \"They {} deeply about the problem.\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "for word, context in zip(words, contexts):\n",
        "    context = context.format(word)\n",
        "    cosine_sim, euclidean_dist = evaluate_transformation(word, context, \"tense_change\")\n",
        "    results.append((word, cosine_sim, euclidean_dist))\n",
        "\n",
        "# Analyze results\n",
        "for word, cosine_sim, euclidean_dist in results:\n",
        "    print(f\"{word}: Cosine Similarity = {cosine_sim:.4f}, Euclidean Distance = {euclidean_dist:.4f}\")\n",
        "\n",
        "# Calculate average performance\n",
        "avg_cosine = np.mean([r[1] for r in results])\n",
        "avg_euclidean = np.mean([r[2] for r in results])\n",
        "print(f\"\\nAverage: Cosine Similarity = {avg_cosine:.4f}, Euclidean Distance = {avg_euclidean:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG5ZADUbwzhg",
        "outputId": "9a807e55-a266-4d23-e74d-fd8bc6283e48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "house: Cosine Similarity = 0.0312, Euclidean Distance = 157.3598\n",
            "car: Cosine Similarity = 0.0016, Euclidean Distance = 148.0092\n",
            "run: Cosine Similarity = 0.0344, Euclidean Distance = 157.0768\n",
            "eat: Cosine Similarity = 0.0218, Euclidean Distance = 144.7155\n",
            "think: Cosine Similarity = 0.0079, Euclidean Distance = 150.5001\n",
            "\n",
            "Average: Cosine Similarity = 0.0194, Euclidean Distance = 151.5323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer function F from similar tense changes"
      ],
      "metadata": {
        "id": "v8cpVjc85slF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, context):\n",
        "    # Tokenize the full context\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\")\n",
        "\n",
        "    # Tokenize the word alone\n",
        "    word_tokens = tokenizer.tokenize(word)\n",
        "\n",
        "    # Get tokens for the context\n",
        "    context_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "\n",
        "    # Find the position of the word in the context\n",
        "    start_index = None\n",
        "    for i, token in enumerate(context_tokens):\n",
        "        if token in word_tokens or any(wt in token for wt in word_tokens):\n",
        "            start_index = i\n",
        "            break\n",
        "\n",
        "    if start_index is None:\n",
        "        raise ValueError(f\"Could not find '{word}' or any of its subwords in the context\")\n",
        "\n",
        "    # Get the embedding\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    return outputs.last_hidden_state[0, start_index, :].numpy()"
      ],
      "metadata": {
        "id": "j1RybUmV5rnE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a larger dataset\n",
        "words_and_contexts = [\n",
        "    (\"run\", \"They {} quickly in the park.\"),\n",
        "    (\"eat\", \"We {} delicious food at the restaurant.\"),\n",
        "    (\"think\", \"She {} deeply about the problem.\"),\n",
        "    (\"play\", \"Children {} happily in the playground.\"),\n",
        "    (\"work\", \"He {} diligently on the project.\"),\n",
        "    (\"study\", \"Students {} hard for their exams.\"),\n",
        "    (\"dance\", \"The couple {} gracefully at the ball.\"),\n",
        "    (\"sing\", \"The choir {} beautifully during the concert.\"),\n",
        "    (\"write\", \"The author {} a new novel every year.\"),\n",
        "    (\"read\", \"She {} interesting books in her free time.\"),\n",
        "    (\"swim\", \"They {} in the ocean every summer.\"),\n",
        "    (\"cook\", \"He {} delicious meals for his family.\"),\n",
        "    (\"paint\", \"The artist {} stunning landscapes.\"),\n",
        "    (\"teach\", \"She {} mathematics at the university.\"),\n",
        "    (\"build\", \"The company {} new offices downtown.\"),\n",
        "    (\"grow\", \"These plants {} well in sunlight.\"),\n",
        "    (\"fly\", \"The birds {} south for the winter.\"),\n",
        "    (\"sleep\", \"The baby {} peacefully in the crib.\"),\n",
        "    (\"laugh\", \"We {} at the comedian's jokes.\"),\n",
        "    (\"cry\", \"The child {} when he lost his toy.\"),\n",
        "    (\"house\", \"The {} is spacious and comfortable.\"),\n",
        "    (\"car\", \"The {} is fast and efficient.\"),\n",
        "    (\"tree\", \"The {} is tall and provides shade.\"),\n",
        "    (\"book\", \"The {} is interesting and informative.\"),\n",
        "    (\"computer\", \"The {} is powerful and user-friendly.\"),\n",
        "    (\"phone\", \"The {} is sleek and has many features.\"),\n",
        "    (\"chair\", \"The {} is comfortable and well-designed.\"),\n",
        "    (\"table\", \"The {} is sturdy and spacious.\"),\n",
        "    (\"picture\", \"The {} is beautiful and well-framed.\"),\n",
        "    (\"window\", \"The {} is large and lets in plenty of light.\"),\n",
        "    (\"happy\", \"She {} about her recent promotion.\"),\n",
        "    (\"sad\", \"He {} about the loss of his pet.\"),\n",
        "    (\"excited\", \"They {} about the upcoming vacation.\"),\n",
        "    (\"tired\", \"I {} after working long hours.\"),\n",
        "    (\"hungry\", \"We {} before dinner time.\"),\n",
        "    (\"thirsty\", \"She {} after the long run.\"),\n",
        "    (\"angry\", \"He {} about the unfair treatment.\"),\n",
        "    (\"surprised\", \"They {} by the unexpected news.\"),\n",
        "    (\"confused\", \"I {} by the complex instructions.\"),\n",
        "    (\"worried\", \"She {} about her upcoming exam.\")\n",
        "]"
      ],
      "metadata": {
        "id": "xO5A5re14iUy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, context):\n",
        "    # Tokenize the full context\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\")\n",
        "\n",
        "    # Tokenize the word alone\n",
        "    word_tokens = tokenizer.tokenize(word)\n",
        "\n",
        "    # Get word IDs for the context\n",
        "    word_ids = inputs.word_ids()[1:-1]  # Exclude special tokens\n",
        "\n",
        "    # Find the position of the word in the context\n",
        "    start_index = None\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        if word_id is not None and inputs.tokens()[i+1] in word_tokens:\n",
        "            start_index = i + 1  # +1 to account for the [CLS] token\n",
        "            break\n",
        "\n",
        "    if start_index is None:\n",
        "        raise ValueError(f\"Could not find '{word}' or any of its subwords in the context\")\n",
        "\n",
        "    # Get the embedding\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    return outputs.last_hidden_state[0, start_index, :].numpy()\n",
        "\n",
        "# Test the function\n",
        "test_word = \"house\"\n",
        "test_context = \"The house is spacious and comfortable.\"\n",
        "try:\n",
        "    embedding = get_embedding(test_word, test_context)\n",
        "    print(f\"Successfully got embedding for '{test_word}'\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Now let's use this function in our main loop\n",
        "X = []  # Original embeddings\n",
        "y = []  # Transformed embeddings\n",
        "\n",
        "for word, context in words_and_contexts:\n",
        "    if word in [\"house\", \"car\", \"tree\", \"book\", \"computer\", \"phone\", \"chair\", \"table\", \"picture\", \"window\"]:\n",
        "        context = context.format(word)\n",
        "    else:\n",
        "        context = context.format(word)\n",
        "\n",
        "    try:\n",
        "        original_embedding = get_embedding(word, context)\n",
        "        transformed_context = context_transformation(context, \"tense_change\")\n",
        "        transformed_embedding = get_embedding(word, transformed_context)\n",
        "\n",
        "        X.append(original_embedding)\n",
        "        y.append(transformed_embedding)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error processing word '{word}': {str(e)}\")\n",
        "        continue\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Continue with the rest of your script (training models, evaluating, etc.)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a linear model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Train a neural network model\n",
        "nn_model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000)\n",
        "nn_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the models\n",
        "def evaluate_model(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    cosine_sims = [np.dot(y[i], y_pred[i]) / (np.linalg.norm(y[i]) * np.linalg.norm(y_pred[i])) for i in range(len(y))]\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    return np.mean(cosine_sims), mse\n",
        "\n",
        "print(\"Linear Model Performance:\")\n",
        "print(\"Training:\", evaluate_model(linear_model, X_train, y_train))\n",
        "print(\"Testing:\", evaluate_model(linear_model, X_test, y_test))\n",
        "\n",
        "print(\"\\nNeural Network Model Performance:\")\n",
        "print(\"Training:\", evaluate_model(nn_model, X_train, y_train))\n",
        "print(\"Testing:\", evaluate_model(nn_model, X_test, y_test))\n",
        "\n",
        "# Test on new words\n",
        "new_words_and_contexts = [\n",
        "    (\"jump\", \"Athletes {} over high bars.\"),\n",
        "    (\"smell\", \"The flowers {} wonderful in the garden.\"),\n",
        "    (\"understand\", \"Students {} the complex theory.\"),\n",
        "    (\"believe\", \"He {} in the power of positive thinking.\"),\n",
        "    (\"mountain\", \"The {} is covered in snow year-round.\")\n",
        "]\n",
        "\n",
        "for new_word, new_context in new_words_and_contexts:\n",
        "    #new_context = new_context.format(\"is\" if new_word == \"mountain\" else new_word)\n",
        "    new_context = new_context.format(new_word)\n",
        "    new_embedding = get_embedding(new_word, new_context)\n",
        "    new_transformed_context = context_transformation(new_context, \"tense_change\")\n",
        "    actual_transformed_embedding = get_embedding(new_word, new_transformed_context)\n",
        "\n",
        "    linear_prediction = linear_model.predict([new_embedding])[0]\n",
        "    nn_prediction = nn_model.predict([new_embedding])[0]\n",
        "\n",
        "    print(f\"\\nNew word '{new_word}':\")\n",
        "    print(f\"Linear Model Prediction - Cosine Similarity: {np.dot(actual_transformed_embedding, linear_prediction) / (np.linalg.norm(actual_transformed_embedding) * np.linalg.norm(linear_prediction)):.4f}\")\n",
        "    print(f\"Neural Network Prediction - Cosine Similarity: {np.dot(actual_transformed_embedding, nn_prediction) / (np.linalg.norm(actual_transformed_embedding) * np.linalg.norm(nn_prediction)):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22d6r_H-yLVA",
        "outputId": "084f1fe0-33b9-499d-fa57-43f4b74a2e68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully got embedding for 'house'\n",
            "Error processing word 'surprised': Could not find 'surprised' or any of its subwords in the context\n",
            "Linear Model Performance:\n",
            "Training: (1.0, 8.0217185e-14)\n",
            "Testing: (0.7558552, 0.10528054)\n",
            "\n",
            "Neural Network Model Performance:\n",
            "Training: (0.9871122, 0.005890542)\n",
            "Testing: (0.6972883, 0.12724763)\n",
            "\n",
            "New word 'jump':\n",
            "Linear Model Prediction - Cosine Similarity: 0.6447\n",
            "Neural Network Prediction - Cosine Similarity: 0.5568\n",
            "\n",
            "New word 'smell':\n",
            "Linear Model Prediction - Cosine Similarity: 0.8047\n",
            "Neural Network Prediction - Cosine Similarity: 0.7472\n",
            "\n",
            "New word 'understand':\n",
            "Linear Model Prediction - Cosine Similarity: 0.6854\n",
            "Neural Network Prediction - Cosine Similarity: 0.6126\n",
            "\n",
            "New word 'believe':\n",
            "Linear Model Prediction - Cosine Similarity: 0.6890\n",
            "Neural Network Prediction - Cosine Similarity: 0.6142\n",
            "\n",
            "New word 'mountain':\n",
            "Linear Model Prediction - Cosine Similarity: 0.6510\n",
            "Neural Network Prediction - Cosine Similarity: 0.5849\n"
          ]
        }
      ]
    }
  ]
}